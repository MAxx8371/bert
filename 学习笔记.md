word2vec对于相同的token采用同一个预训练的embedding

* feature-based
* fine-tuning

masked-language-model objection能够用于训练深度双向transformer



* 隐藏层dropout
* attention层dropout

use_one_hot_embeddings：是否使用one-hot embedding，有什么用？

If True, use one-hot method for word  embeddings. If False, use `tf.gather()`.



获取embedding后就使用了一个layer_normal 和  dropout

函数get_assignment_map_from_checkpoint有什么用？

gelu

maksed LM训练时，是对AB句都attention吗？

# Modeling

* ```python
  def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
      ''' 如何理解？
          assignment_map[name] = name
      	initialized_variable_names[name] = 1   
      	initialized_variable_names[name + ":0"] = 1
  	'''
      pass
  ```

* 



# Create pretrain data

* ```python
  def create_training_instances(input_files, tokenizer, max_seq_length,
                                dupe_factor, short_seq_prob, masked_lm_prob,
                                max_predictions_per_seq, rng):
      '''
      	1.对documents每句话分词，存入all_documents
      	2.调用create_instances_from_document创建training_instance
      '''
      pass
  ```

* ```python
  def create_instances_from_document(
      all_documents, document_index, max_seq_length, short_seq_prob,
      masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
      ''' 使用第document_index个样本创建instances
      	1. 从document中选择两个列表 tokens_a，tokens_b；
      	2. 使用 tokens_a和tokens_b 生成 tokens和segment_ids 用于 NSP
      	3. 调用create_masked_lm_predictions，利用tokens生成masker_LM标签
      	4. 实例化TrainingInstance类对象
      '''
      pass
  ```

* ```python
  def create_masked_lm_predictions(tokens, masked_lm_prob,
                                   max_predictions_per_seq, vocab_words, rng):
      '''
      	1. 所有token的index放入cand_indexes，打乱
      	2. 顺序选择num_to_predict个index做遮掩
      	3. output_tokens(masked tokens), masked_lm_positions, masked_lm_labels  全部为list
      '''
      pass
  ```

* ```python
  def write_instance_to_example_files(instances, tokenizer, max_seq_length,
                                      max_predictions_per_seq, output_files):
      '''
      	1. 对每个instance的input_ids、segment_ids，padding到max_seq_length并构造input_mask
      	2. 对每个instance的masked_lm_positions、masked_lm_ids，padding到max_predictions_per_seq并构造masked_lm_weights
      	3. 实例化tf.train.Example并写入目标文件（可以考虑使用多进程加速）
      '''
      pass
  ```

# Tokenization

* **convert_to_unicode**：将输入转化为utf-8格式

* **load_vocab**：加载词表

* **convert_by_vocab**：将 tokens/ids 转化为 ids/tokens

* **_is_punctuation**：判断是否是标点符号

  * 所有非数字和字母都被看做标点符号

* **_is_control** ：判断是否为**控制字符**

  * \t, \n, \r 被认为是空格字符，不属于控制字符

* **_is_whitespace** ：判断是否是空格字符

* ```python
  class BasicTokenizer(object):
      def _clean_text(self, text):
          '''去除各种奇怪字符，包括\x00, �, 控制字符'''
          pass
      def _is_chinese_char(self, cp):
          '''根据码位判断是否是中文'''
          pass
      def _tokenize_chinese_chars(self, text):
          '''在中文字符前后添加空格'''
          pass
      def _run_strip_accents(self, text):
          '''去除重音符号，例如 "āóǔè" 变为 "aoue" '''
          pass
      def _run_split_on_punc(self, text):
          '''标签符号分词'''
          pass
      def tokenize(self, text):
          '''转为unicode; 去除奇异字符; 中文分词; 空格分词; 去除重音，标点分词; 空格分词'''
          pass
  ```

* ```python
  class WordpieceTokenizer(object):
      '''在BT结果的基础上进行再一次切分，得到子词（subword，以##开头），词汇表在此时引入'''
  	'''e.g.: input = "unaffable", output = ["un", "##aff", "##able"]'''
      def tokenize(self, text):
          '''按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长'''
  ```

* ```python
  class FullTokenizer(object):
      def tokenize(self, text):
          '''对text先用BasicTokenizer分词为word，再用WordpieceTokenizer分词为subword'''
  ```

# Optimization

* ```python
  class AdamWeightDecayOptimizer(tf.train.Optimizer):
      '''L2正则 + Adam优化器'''
      def apply_gradients(self, grads_and_vars, global_step=None, name=None):
          '''定义m,v; 计算next_m, next_v; L2正则; 更新参数（包含m,v）'''
          pass
      def _do_use_weight_decay(self, param_name):
          '''判断输入的param是否进行L2正则'''
          pass
      def _get_variable_name(self, param_name):
          '''正则匹配的方式获取参数名'''
          pass
  def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):
      '''定义train_op'''
      '''定义learning_rate(warm-up, poly_decay); 定义optimizer; 定义train_op'''
  ```

# run_pretraining  

* ```python
  def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
                           label_ids, label_weights):
      '''
          1. 调用gather_indexes生成input_tensor
          2. input_tensor经过一个全连接层，与output_weights相乘生成logits
          3. label_ids, label_weights, logits计算masked_lm_loss
      '''
  ```

* ```python
  def gather_indexes(sequence_tensor, positions):
    '''获得指定位置上的张量
    		sequence_tensor: [batch_size, seq_length, width]
    		positions: [batch_size, masked_length]
    		output: [batch_size, masked_length, width]
    '''
  	pass
  ```

* ```python
  def get_next_sentence_output(bert_config, input_tensor, labels):
      '''Get loss and log probs for the next sentence prediction.(简单二分类)'''
  	pass
  ```

* ```python
  def input_fn(params):
      '''
      	1. 从文件读取TFRecord （parallel_interleave并行交织）
      	2. TFRecord解析为feature
      '''
  ```