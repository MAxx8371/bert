word2vec对于相同的token采用同一个预训练的embedding

* feature-based
* fine-tuning

masked-language-model objection能够用于训练深度双向transformer



* 隐藏层dropout
* attention层dropout

use_one_hot_embeddings：是否使用one-hot embedding，有什么用？

If True, use one-hot method for word  embeddings. If False, use `tf.gather()`.



获取embedding后就使用了一个layer_normal 和  dropout



函数get_assignment_map_from_checkpoint有什么用？