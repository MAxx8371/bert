word2vec对于相同的token采用同一个预训练的embedding

* feature-based
* fine-tuning

masked-language-model objection能够用于训练深度双向transformer

* 隐藏层dropout
* attention层dropout

use_one_hot_embeddings：是否使用one-hot embedding，有什么用？

If True, use one-hot method for word  embeddings. If False, use `tf.gather()`.

函数get_assignment_map_from_checkpoint有什么用？

maksed LM训练时，是对AB句都attention吗？

# Modeling

* ```python
  def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
      ''' 如何理解？
          assignment_map[name] = name
      	initialized_variable_names[name] = 1   
      	initialized_variable_names[name + ":0"] = 1
  	'''
      pass
  ```

* ```python
  class BertConfig(object):
      ''' Bert BASE
      	hidden_size=768(H)
      	num_hidden_layers=12(L)
      	num_attention_heads=12(A)
      	
      	intermediate_size: The size of the feed-forward layer
      '''
  ```

* ```python
  class BertModel(object):
      '''
      	pooler_output: 取[cls]对应的输出经过一层全连接层后的结果（[batch_size, width]
      	sequence_output: [batch_size, seq_length, width]
      '''
      pass
  ```

* ```python
  def embedding_lookup(input_ids,
                       vocab_size,
                       embedding_size=128,
                       initializer_range=0.02,
                       word_embedding_name="word_embeddings",
                       use_one_hot_embeddings=False):
      ''' embedding_size == hidden_size
      	input_ids: [batch_size, seq_length]
      	1. flat_input_ids -> tf.one_hot -> tf.matmul ->tf.resahpe
      	2. flat_input_ids -> tf.gather -> tf.reshape
      	output: [batch_size, seq_length, embedding_size]
      '''
      # This function assumes that the input is of shape [batch_size, seq_length,
      # num_inputs]，这里的num_inputs表示什么，有不是1的情况吗？
  ```

* ```python
  def embedding_postprocessor(input_tensor,
                              use_token_type=False,
                              token_type_ids=None,
                              token_type_vocab_size=16,
                              token_type_embedding_name="token_type_embeddings",
                              use_position_embeddings=True,
                              position_embedding_name="position_embeddings",
                              initializer_range=0.02,
                              max_position_embeddings=512,
                              dropout_prob=0.1):
      '''
      	token_type_ids: segment_ids
      	token_type_embedding: flat_token_type_ids-> tf.one_hot-> tf.matmul
      	position_embeddings: tf.slice -> tf.reshape(position_broadcast_shape)
      	layer_norm_and_dropout(获取embedding后使用了layer_normal和 dropout)
      '''
      pass
  ```

* ```python
  def attention_layer(from_tensor,
                      to_tensor,
                      attention_mask=None,
                      num_attention_heads=1,
                      size_per_head=512,
                      query_act=None,
                      key_act=None,
                      value_act=None,
                      attention_probs_dropout_prob=0.0,
                      initializer_range=0.02,
                      do_return_2d_tensor=False,
                      batch_size=None,
                      from_seq_length=None,
                      to_seq_length=None):
      ''' self-attention:
      	query, key, value: reshape-> transpose-> [batch_size, num_heads, seq_length, width]
          mask: [batch_size, 1, seq_length, seq_length] 
          prob = tf.matmul(query, key, transpose=True)
          context_layer = tf.matmul(prob, value) -> transpose-> reshape
      '''
  ```

* ```python
  def transformer_model(input_tensor,
                        attention_mask=None,
                        hidden_size=768,
                        num_hidden_layers=12,
                        num_attention_heads=12,
                        intermediate_size=3072,
                        intermediate_act_fn=gelu,
                        hidden_dropout_prob=0.1,
                        attention_probs_dropout_prob=0.1,
                        initializer_range=0.02,
                        do_return_all_layers=False):
      '''多层transformer的encoder'''
      for layer_idx in range(num_hidden_layers):
          attention_output = attention_layer()
  ```

* ```python
  def create_attention_mask_from_input_mask(from_tensor, to_mask):
  	'''为什么要分为from_tensor和to_tensor？'''
      '''self-attention时，from_tensor==to_tensor；其他形式的attention则不同'''
      pass
  ```

* gelu()



# Create pretrain data

* ```python
  def create_training_instances(input_files, tokenizer, max_seq_length,
                                dupe_factor, short_seq_prob, masked_lm_prob,
                                max_predictions_per_seq, rng):
      '''
      	1.对documents每句话分词，存入all_documents
      	2.调用create_instances_from_document创建training_instance
      '''
      pass
  ```

* ```python
  def create_instances_from_document(
      all_documents, document_index, max_seq_length, short_seq_prob,
      masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
      ''' 使用第document_index个样本创建instances
      	1. 从document中选择两个列表 tokens_a，tokens_b；
      	2. 使用 tokens_a和tokens_b 生成 tokens和segment_ids 用于 NSP
      	3. 调用create_masked_lm_predictions，利用tokens生成masker_LM标签
      	4. 实例化TrainingInstance类对象
      '''
      pass
  ```

* ```python
  def create_masked_lm_predictions(tokens, masked_lm_prob,
                                   max_predictions_per_seq, vocab_words, rng):
      '''
      	1. 所有token的index放入cand_indexes，打乱
      	2. 顺序选择num_to_predict个index做遮掩
      	3. output_tokens(masked tokens), masked_lm_positions, masked_lm_labels  全部为list
      '''
      pass
  ```

* ```python
  def write_instance_to_example_files(instances, tokenizer, max_seq_length,
                                      max_predictions_per_seq, output_files):
      '''
      	1. 对每个instance的input_ids、segment_ids，padding到max_seq_length并构造input_mask
      	2. 对每个instance的masked_lm_positions、masked_lm_ids，padding到max_predictions_per_seq并构造masked_lm_weights
      	3. 实例化tf.train.Example并写入目标文件（可以考虑使用多进程加速）
      '''
      pass
  ```

# Tokenization

* **convert_to_unicode**：将输入转化为utf-8格式

* **load_vocab**：加载词表

* **convert_by_vocab**：将 tokens/ids 转化为 ids/tokens

* **_is_punctuation**：判断是否是标点符号

  * 所有非数字和字母都被看做标点符号

* **_is_control** ：判断是否为**控制字符**

  * \t, \n, \r 被认为是空格字符，不属于控制字符

* **_is_whitespace** ：判断是否是空格字符

* ```python
  class BasicTokenizer(object):
      def _clean_text(self, text):
          '''去除各种奇怪字符，包括\x00, �, 控制字符'''
          pass
      def _is_chinese_char(self, cp):
          '''根据码位判断是否是中文'''
          pass
      def _tokenize_chinese_chars(self, text):
          '''在中文字符前后添加空格'''
          pass
      def _run_strip_accents(self, text):
          '''去除重音符号，例如 "āóǔè" 变为 "aoue" '''
          pass
      def _run_split_on_punc(self, text):
          '''标签符号分词'''
          pass
      def tokenize(self, text):
          '''转为unicode; 去除奇异字符; 中文分词; 空格分词; 去除重音，标点分词; 空格分词'''
          pass
  ```

* ```python
  class WordpieceTokenizer(object):
      '''在BT结果的基础上进行再一次切分，得到子词（subword，以##开头），词汇表在此时引入'''
  	'''e.g.: input = "unaffable", output = ["un", "##aff", "##able"]'''
      def tokenize(self, text):
          '''按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长'''
  ```

* ```python
  class FullTokenizer(object):
      def tokenize(self, text):
          '''对text先用BasicTokenizer分词为word，再用WordpieceTokenizer分词为subword'''
  ```

# Optimization

* ```python
  class AdamWeightDecayOptimizer(tf.train.Optimizer):
      '''L2正则 + Adam优化器'''
      def apply_gradients(self, grads_and_vars, global_step=None, name=None):
          '''定义m,v; 计算next_m, next_v; L2正则; 更新参数（包含m,v）'''
          pass
      def _do_use_weight_decay(self, param_name):
          '''判断输入的param是否进行L2正则'''
          pass
      def _get_variable_name(self, param_name):
          '''正则匹配的方式获取参数名'''
          pass
  def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):
      '''定义train_op'''
      '''定义learning_rate(warm-up, poly_decay); 定义optimizer; 定义train_op'''
  ```

# run_pretraining  

* ```python
  def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
                           label_ids, label_weights):
      '''
          1. 调用gather_indexes生成input_tensor
          2. input_tensor经过一个全连接层，与output_weights相乘生成logits
          3. label_ids, label_weights, logits计算masked_lm_loss
      '''
  ```

* ```python
  def gather_indexes(sequence_tensor, positions):
    '''获得指定位置上的张量
    		sequence_tensor: [batch_size, seq_length, width]
    		positions: [batch_size, masked_length]
    		output: [batch_size, masked_length, width]
    '''
  	pass
  ```

* ```python
  def get_next_sentence_output(bert_config, input_tensor, labels):
      '''Get loss and log probs for the next sentence prediction.(简单二分类)'''
  	pass
  ```

* ```python
  def input_fn(params):
      '''
      	1. 从文件读取TFRecord （parallel_interleave并行交织）
      	2. TFRecord解析为feature
      '''
  ```